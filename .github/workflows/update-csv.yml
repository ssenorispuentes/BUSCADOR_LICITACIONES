name: Ejecutar scraping y actualizar CSV

on:
  schedule:
    - cron: '0 6 * * *'  # Todos los dias a las 06:00 UTC
  workflow_dispatch:

jobs:
  run-scraper:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout del repositorio
      uses: actions/checkout@v3
      with:
        persist-credentials: false

    - name: Instalar dependencias
      run: |
        rm -rf ~/.cache/pip
        pip install --upgrade pip
        pip install beautifulsoup4  # Fuerza instalaci√≥n directa
        pip install -r requirements.txt
        python -m spacy download es_core_news_sm
        python -m nltk.downloader stopwords

    - name: Verificar instalaci√≥n y acceso a bs4
      run: |
        echo "üßê Verificando entorno..."
        which python
        which pip
        python --version
        pip --version
        echo "üì¶ Verificando instalaci√≥n de bs4..."
        pip show beautifulsoup4
        python -c "import bs4; print('‚úÖ bs4 est√° disponible y funcionando')"

    - name: Ejecutar script de scraping
      run: |
        echo "üöÄ Ejecutando main_scraping.py"
        python main_scraping.py --usar_scraping

    - name: Subir CSV actualizado al repositorio
      run: |
        git config user.name "github-actions"
        git config user.email "github-actions@github.com"
        git add datos_licitaciones_final/licitaciones.csv
        git commit -m "Actualizar CSV autom√°ticamente"
        git push https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }} HEAD:main

